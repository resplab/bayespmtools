---
title: "Bayespmtools Package Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bayespmtools_tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(bayespmtools)
```

## Introduction

Current sample size calculations for external validation of risk prediction models require researchers to have fixed values of assumed model performance metrics alongside target precision levels. Using a Bayesian framework enables researchers to have more flexibility for sample size rules, including being able to include uncertainty in model performance in sample size calculations, and specify targets based on expected precision, assurance probabilities, and Value of Information (VoI).

The BayesPMTools R package incorporates the implementation of our proposed Bayesian approach towards Riley's multi-criteria sample size calculations. Details of this approach are provided in <https://arxiv.org/pdf/2504.15923>.

This tutorial overviews two core functions in this R package: ***bpm_valsamp***, and ***bpm_valprec***. This tutorial assumes the BayesPMTools package is installed on your computer. Please refer to <https://github.com/resplab/bayespmtools> for instructions on more instructions on how to install and use this package.

### The ISARIC Model

As a case study, we present sample size calculations for the ISARIC 4C model. The ISARIC 4C Model is a risk prediction model for predicting deterioration in patients hospitalized from COVID-19 @gupta2021isaric. The investigators used electronic hospital records from all 9 health regions of the UK to develop and validate this model. London was left out for external validation while the other 8 regions were used in creating the model.

We assume we plan to validate this study in the London region. Based on the exchangeability assumption, the performance of the model in the London region will be a random draw from the predictive distribution of performance metrics observed across the other 8 regions of the UK.

### Computing Evidence and Specifying Targets

The values in the evidence are extracted from meta-analysis of the original report for the ISARIC 4C model, which produced these values based on predictive distribution of the internal-external validation results.

The Bayesian sample size calculation approach requires specifying prior distribution on 1) outcome prevalence, 2) c-statistic, 3) calibration slope, 4) one of calibration intercept, observed-to-expected (O/E) ratio, or mean calibration (difference between average observed and predicted risks - which is the one we use for this example). These distributions are derived from a meta-analysis of internal-external validation results from the original study. Details of this meta-analysis are provided elsewhere @gupta2021isaric. Here, we directly use the predictive distributions from this meta-analysis.

```{r set-seed}
# Settings the random number seed for reproducibility
set.seed(123)
```

Specified evidence should be a list, with exact naming per below. Note that we are specifying evidence via mean and sd. Other ways of doing so is also possible. Please refer to the help documents for the related functions.

```{r load-evidence}
evidence <- list(prev=list(type="beta", mean=0.428, sd=0.030),
                 cstat=list(type="logitnorm", mean=0.761, sd=0.006),
                 cal_mean=list(type="norm", mean=-0.009, sd=0.125),
                 cal_slp=list(type="norm", mean=0.995, sd = 0.024))
```

### Targets

Targets are specified based the same as the method proposed in @riley2021minsample, for comparability. The target 95% confidence interval widths are as follows: C-Statistic: 0.10, O/E ratio: 0.22, Calibration Slope: 0.3.

The expected confidence intervals widths (eciw) only require a target confidence interval width. Meanwhile, the quantile confidence interval widths (qciw) require the target confidence interval width to be specified along with the desired assurance (quantile). In our example we use a quantile of 0.9 for 90% assurance.

The assurance.nb parameter is the Optimality Assurance defined as the probability that we will correctly identify the strategy that has the highest true net benefit based on the validation data. In this example it is set at 0.9.

```{r targets-samp}
targets_samp <- list(eciw.cstat=0.1,
                eciw.cal_oe=0.22,
                eciw.cal_slp=0.30,
                qciw.cstat=c(0.1, 0.9),
                qciw.cal_oe=c(0.22, 0.9),
                qciw.cal_slp=c(0.30,0.9),
                assurance.nb=0.9)
```

### Sample size calculator (*bpm_valsamp*())

This is the main function call that computes the sample size requirements based on our evidence and targets. It returns the required sample size for external validation for each parameter requested in the targets.

```{r eval=FALSE}
res <- bpm_valsamp(evidence=evidence, #Evidence as a list
                   dist_type="logitnorm", #Distribution type for calibrated risks
                   method="sample", #Sample based or tw-level ("2s") method
                   targets=targets_samp, #Targets (as specified above)
                   n_sim=10000, #Number of Monte Carlo simulations
                   threshold=0.2) #Risk threshold for NB VoI calculations

```

```{r print-samp-results}
#Print results
res <- readRDS("precomputed/samp_result.rds")
print(res$N)
```

As can be seen above, different components require different sample size estimates. The largest sample size is 1171, dictated by on the assurance desired for the calibration slope criterion.

### Computing precision / VoI for a given sample size (*bpm_valprec*())

This function returns the parameter values based on the given sample sizes and targets. It works as an inverse to the ***bpm_valsamp*** function.

For this example, we will be using the outputs from ***bpm_valsamp*** to verify that the 95% CI widths and VoI values for sample size components derived from ***bpm_valprec*** are indeed close to the desired ones. As such, we use the same evidence element defined earlier, and create a new target element *target_prec*, that contains the same parameters *target_samp* but defined as T for present eciw targets, or 0.9 for present qciw targets.

The *N_prec* variable includes the output sample sizes from ***bpm_valsamp***.

```{r}
N_prec = c(363, 424, 1071, 401, 521, 1171, 306)

targets_prec = list(eciw.cstat = T, eciw.cal_oe = T, eciw.cal_slp = T, qciw.cstat = 0.9, qciw.cal_oe = 0.9, qciw.cal_slp = 0.9, assurance.nb=T)

```

```{r eval=FALSE}
prec <- bpm_valprec(
  N = N_prec, #Sample sizes
  evidence = evidence, #Evidence as a list
  dist_type="logitnorm", #Distribution type for calibrated risks
  method="sample", #Sample based or tw-level ("2s") method
  targets = targets_prec, #Targets specified above
  n_sim = 10000, #Number of Monte Carlo simulations
  threshold=0.2) #Risk threshold for NB VoI calculations

```

### Validating and Understanding the Outputs

Given our output, below we can validate the results of ***bpm_valsamp*** by comparing our output of ***bpm_valprec***to using our *targets_samp* and *N_prec* variable.

Our target confidence interval width passed into ***bpm_valsamp*** for *eciw.cstat* was 0.1. And the resulting sample size we calculated to reach this target was 351.

Notice that 351 is the first element in our *N_prec* variable that is being passed into ***bpm_valprec***. Now we can look at the first element in our output for ***bpm_valprec*** which corresponds to *eciw.cstat* and see that the calculated precision is indeed 0.1 which is exactly the same as the targets specified.

Additionally, we can continue verifying the remaining parameters. Notice that the second element in the ***bpm_valprec*** output for *eciw.cal_oe* is 0.22 which matches target specified earlier for *eciw.cal_oe* in *targets_samp.*

assurance0 is the assurance before any validation studies given our current uncertainties. It is a baseline assurance based on our evidence and not dependent on any targets.

```{r include = FALSE}
### This code will not show in the vignette
### Used to round the results so that the tutorial is easier to follow
prec <- readRDS("precomputed/prec_result.rds")

prec$eciw$cstat <- round(prec$eciw$cstat, 3)
prec$eciw$cal_oe <- round(prec$eciw$cal_oe, 3)
prec$eciw$cal_slp <- round(prec$eciw$cal_slp, 3)

prec$qciw$cstat <- round(prec$qciw$cstat, 3)
prec$qciw$cal_oe <- round(prec$qciw$cal_oe, 3)
prec$qciw$cal_slp <- round(prec$qciw$cal_slp, 3)

prec$assurance$assurance0 <- round(prec$assurance$assurance0, 3)
prec$assurance$assurance <- round(prec$assurance$assurance , 3)
```

```{r compare_results}

results <- data.frame(sample_size = N_prec, eciw = prec$eciw, qciw = prec$qciw, prec$assurance)

print(results)
```

### References
